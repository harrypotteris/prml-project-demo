{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Classifying fruits using both color histograms and metadata as input features to a Naive Bayes model."
      ],
      "metadata": {
        "id": "ymmwm8lRAgzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztlsf5Z9AK0I",
        "outputId": "cd09a30d-0fa6-4f08-eff6-4c8a0b77fe87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8542275286845337\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93       157\n",
            "           1       0.35      0.58      0.44       164\n",
            "           2       0.54      0.41      0.46       148\n",
            "           3       0.92      0.87      0.89       160\n",
            "           4       0.91      0.99      0.95       164\n",
            "           5       0.63      0.83      0.72       161\n",
            "           6       0.84      1.00      0.91       164\n",
            "           7       0.43      0.66      0.52       152\n",
            "           8       0.67      0.87      0.76       164\n",
            "           9       0.66      0.77      0.71       164\n",
            "          10       0.43      0.30      0.35       144\n",
            "          11       1.00      1.00      1.00       166\n",
            "          12       0.42      0.42      0.42       164\n",
            "          13       1.00      1.00      1.00       219\n",
            "          14       0.91      0.98      0.94       234\n",
            "          15       1.00      0.88      0.94       164\n",
            "          16       0.52      0.64      0.57       143\n",
            "          17       1.00      1.00      1.00       166\n",
            "          18       0.99      1.00      1.00       166\n",
            "          19       1.00      1.00      1.00       152\n",
            "          20       0.88      0.77      0.82       166\n",
            "          21       0.98      0.35      0.51       150\n",
            "          22       0.99      0.97      0.98       154\n",
            "          23       1.00      1.00      1.00        47\n",
            "          24       0.68      0.90      0.78       166\n",
            "          25       1.00      0.98      0.99       164\n",
            "          26       0.98      0.99      0.99       164\n",
            "          27       0.99      0.95      0.97       166\n",
            "          28       1.00      1.00      1.00        50\n",
            "          29       1.00      0.97      0.98       234\n",
            "          30       1.00      1.00      1.00       164\n",
            "          31       1.00      1.00      1.00       246\n",
            "          32       1.00      0.99      0.99       246\n",
            "          33       1.00      1.00      1.00       164\n",
            "          34       1.00      1.00      1.00       164\n",
            "          35       1.00      1.00      1.00       164\n",
            "          36       0.62      0.87      0.73       153\n",
            "          37       1.00      0.93      0.96       166\n",
            "          38       0.99      0.94      0.96       166\n",
            "          39       1.00      0.33      0.50       150\n",
            "          40       1.00      0.51      0.67       154\n",
            "          41       1.00      1.00      1.00        50\n",
            "          42       0.93      1.00      0.96        81\n",
            "          43       0.61      0.68      0.64       130\n",
            "          44       1.00      0.56      0.72       156\n",
            "          45       1.00      1.00      1.00       166\n",
            "          46       1.00      0.51      0.67       156\n",
            "          47       1.00      1.00      1.00        80\n",
            "          48       0.97      0.76      0.85       234\n",
            "          49       0.98      0.62      0.76        99\n",
            "          50       0.95      1.00      0.98       166\n",
            "          51       0.99      1.00      0.99       328\n",
            "          52       0.85      0.87      0.86       164\n",
            "          53       1.00      1.00      1.00       166\n",
            "          54       1.00      0.99      1.00       166\n",
            "          55       1.00      0.89      0.94       164\n",
            "          56       1.00      0.93      0.96       158\n",
            "          57       1.00      1.00      1.00       166\n",
            "          58       1.00      0.98      0.99       164\n",
            "          59       1.00      0.99      1.00       166\n",
            "          60       0.90      1.00      0.95       157\n",
            "          61       1.00      1.00      1.00       166\n",
            "          62       1.00      1.00      1.00       166\n",
            "          63       0.88      1.00      0.94       156\n",
            "          64       0.97      0.85      0.91       157\n",
            "          65       1.00      0.99      1.00       166\n",
            "          66       0.67      1.00      0.80       164\n",
            "          67       1.00      1.00      1.00       166\n",
            "          68       0.95      1.00      0.97       166\n",
            "          69       1.00      1.00      1.00       166\n",
            "          70       1.00      1.00      1.00       166\n",
            "          71       0.90      0.89      0.89       166\n",
            "          72       0.53      0.47      0.50       142\n",
            "          73       0.94      1.00      0.97       102\n",
            "          74       1.00      0.94      0.97       166\n",
            "          75       0.88      0.86      0.87       246\n",
            "          76       1.00      1.00      1.00       164\n",
            "          77       0.01      0.01      0.01       164\n",
            "          78       1.00      1.00      1.00       160\n",
            "          79       1.00      1.00      1.00       218\n",
            "          80       0.09      0.09      0.09       178\n",
            "          81       0.38      0.61      0.47       150\n",
            "          82       0.93      0.99      0.96       155\n",
            "          83       0.83      0.88      0.85       146\n",
            "          84       1.00      1.00      1.00       160\n",
            "          85       0.85      1.00      0.92       164\n",
            "          86       1.00      1.00      1.00       166\n",
            "          87       0.37      0.60      0.46       164\n",
            "          88       1.00      1.00      1.00       246\n",
            "          89       0.53      0.51      0.52       164\n",
            "          90       0.86      0.82      0.84       164\n",
            "          91       0.53      0.19      0.28       232\n",
            "          92       1.00      1.00      1.00        72\n",
            "          93       0.59      0.73      0.65       166\n",
            "          94       0.87      0.98      0.92       234\n",
            "          95       1.00      1.00      1.00       102\n",
            "          96       1.00      0.96      0.98       166\n",
            "          97       0.65      0.45      0.53       222\n",
            "          98       0.76      0.96      0.85       237\n",
            "          99       0.43      0.46      0.45       166\n",
            "         100       0.64      0.83      0.72       166\n",
            "         101       0.66      1.00      0.80       148\n",
            "         102       1.00      1.00      1.00       234\n",
            "         103       0.72      0.93      0.81       222\n",
            "         104       1.00      0.89      0.94       222\n",
            "         105       1.00      0.96      0.98       164\n",
            "         106       0.66      0.98      0.79       164\n",
            "         107       0.88      0.95      0.91       166\n",
            "         108       1.00      0.87      0.93       163\n",
            "         109       1.00      1.00      1.00       166\n",
            "         110       1.00      0.79      0.89       151\n",
            "         111       1.00      1.00      1.00       142\n",
            "         112       0.82      0.78      0.80       304\n",
            "         113       0.33      0.29      0.31       164\n",
            "         114       1.00      1.00      1.00       153\n",
            "         115       0.46      0.32      0.38       150\n",
            "         116       0.42      0.46      0.44       151\n",
            "         117       0.71      0.33      0.45       150\n",
            "         118       0.99      0.91      0.95       150\n",
            "         119       1.00      1.00      1.00       166\n",
            "         120       1.00      1.00      1.00       164\n",
            "         121       1.00      1.00      1.00       166\n",
            "         122       1.00      1.00      1.00       164\n",
            "         123       0.98      1.00      0.99       162\n",
            "         124       0.80      1.00      0.89       164\n",
            "         125       0.68      0.49      0.57       246\n",
            "         126       1.00      0.97      0.98       166\n",
            "         127       1.00      1.00      1.00       166\n",
            "         128       0.98      1.00      0.99       246\n",
            "         129       1.00      1.00      1.00       225\n",
            "         130       1.00      0.98      0.99       246\n",
            "         131       1.00      1.00      1.00       160\n",
            "         132       1.00      1.00      1.00       164\n",
            "         133       0.96      0.68      0.80       228\n",
            "         134       1.00      1.00      1.00       127\n",
            "         135       1.00      1.00      1.00       153\n",
            "         136       0.51      0.59      0.55       158\n",
            "         137       1.00      1.00      1.00       249\n",
            "         138       0.93      0.96      0.94       157\n",
            "         139       1.00      1.00      1.00        80\n",
            "         140       1.00      0.93      0.96        80\n",
            "\n",
            "    accuracy                           0.85     23619\n",
            "   macro avg       0.86      0.85      0.85     23619\n",
            "weighted avg       0.86      0.85      0.85     23619\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[152   0   0 ...   0   0   0]\n",
            " [  0  95   1 ...   0   0   0]\n",
            " [  0   0  60 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 150   0   0]\n",
            " [  0   0   0 ...   0  80   0]\n",
            " [  0   0   0 ...   0   0  74]]\n"
          ]
        }
      ],
      "source": [
        "# 📦 Imports\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 📁 Load data - only histogram files now\n",
        "hist_train = pd.read_csv('/content/colour_Histogram_Training.csv')\n",
        "hist_test = pd.read_csv('/content/colour_Histogram_Testing.csv')\n",
        "\n",
        "# 🎯 Separate features and target (no merging with metadata needed)\n",
        "X_train = hist_train.drop(columns=['filename', 'class'])  # Keep only histogram features\n",
        "y_train = hist_train['class']\n",
        "X_test = hist_test.drop(columns=['filename', 'class'])\n",
        "y_test = hist_test['class']\n",
        "\n",
        "# 🧠 Encode target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# ⚙️ Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 📊 Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train_scaled, y_train_encoded)\n",
        "\n",
        "# 🔍 Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# 📈 Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_encoded, y_pred))"
      ]
    }
  ]
}